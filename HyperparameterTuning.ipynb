{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import flopy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from scipy.interpolate import interp2d\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from flopy.utils.sfroutputfile import SfrFile\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import DecisionTreeRegressor # Import Decision Tree Classifier\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "modws =\"C:/Users/davem/Documents/Thesis/modflow/mfoutput\"\n",
    "modws= \"D:/mfoutput\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68521cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the well (feature) and streamflow (target) data \n",
    "ibound = np.loadtxt('base/ibound_lyr1.txt')            \n",
    "all_well_loc = np.asarray(np.where(ibound == 1)).T\n",
    "\n",
    "#Grab the pickles of ReachQ (streamflow) and well heads\n",
    "ReachQdf = pd.read_pickle('outputs/SS_reachQ1000_test11.pkl')\n",
    "well_head_df= pd.read_pickle('outputs/heads/WellHead_SS1000test11.pkl')\n",
    "\n",
    "## Setting up the head inputs - dropping out the downstream most CH boundary and the river cells \n",
    "river_well_loc = np.where(all_well_loc[:,0] == 25)\n",
    "CHBound_loc = np.where(all_well_loc[:,1] > 47)\n",
    "drop_locs= np.concatenate((river_well_loc, CHBound_loc), axis = 1)\n",
    "\n",
    "well_head_noRiv = well_head_df.drop(drop_locs[0], axis = 1)\n",
    "\n",
    "streamflow = ReachQdf.loc[well_head_noRiv.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eddc31c",
   "metadata": {},
   "source": [
    "### Decision Tree Hyperparameter Tuning \n",
    "\n",
    "Prints out the best hyperparameter combo from the search as a dictionary - which we then use in the other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8266b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the hyperparameters\n",
    "\n",
    "#set this so we can maintain the train test split in the actual run of the model\n",
    "random_state = 29\n",
    "\n",
    "#reach number to use \n",
    "rn = 25\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(well_head_noRiv, streamflow[rn], test_size=0.2, random_state = random_state)\n",
    "\n",
    "#split quality function\n",
    "split_criterion = ['squared_error', 'friedman_mse']\n",
    "\n",
    "#Number of features to consider at every split\n",
    "max_features = [1.0]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in range(4,9)] #14 was the max depth, 3 or greater is recommened, 8 seems high enough \n",
    "#max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "\n",
    "#maximum \n",
    "maxSqrt = int(np.max(y_train)**(1/2))\n",
    "max_leaf_nodes = [int(x) for x in range(50,maxSqrt,int(maxSqrt/10))]\n",
    "max_leaf_nodes.append(None)\n",
    "\n",
    "#min weight fraction leaf - need to look into this one \n",
    "#min_weight_fraction_leaf = [0.0]\n",
    "\n",
    "min_imp_decrease = [int(x) for x in range(5,55,10)]\n",
    "#min_imp_decrease.append(0)\n",
    "#cost complexcity prunning \n",
    "ccp_alpha = [0.0]\n",
    "\n",
    "splitter = ['best']\n",
    "# Create the random grid\n",
    "random_grid = {#'n_estimators': n_estimators,\n",
    "                'criterion' : split_criterion,\n",
    "               #'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              # 'min_weight_fraction_leaf': min_weight_fraction_leaf,\n",
    "                'max_leaf_nodes': max_leaf_nodes,\n",
    "                'min_impurity_decrease':min_imp_decrease}\n",
    "                #'splitter': splitter}\n",
    "print(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "dtr = DecisionTreeRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "dtr_random = RandomizedSearchCV(estimator = dtr, scoring = 'neg_root_mean_squared_error',param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=29, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "dtr_random.fit(X_train, y_train)\n",
    "best_pars_dt = dtr_random.best_params_\n",
    "print(best_pars_dt)\n",
    "\n",
    "best_random = dtr_random.best_estimator_\n",
    "y_pred = best_random.predict(X_test)\n",
    "#well head Tree NSE\n",
    "\n",
    "#label='Prediction to Test Values for Hyperparameter Tuned Tree'\n",
    "plt.scatter(y_test, y_pred)  \n",
    "outmax = np.max(y_test)\n",
    "plt.plot([0, outmax], [0, outmax], color='k', linestyle='-')\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "# plt.xlim(60,100)\n",
    "# plt.ylim(60,100)\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# _, ax = plt.subplots(figsize=(25, 25))\n",
    "# _ = plot_tree(best_random, ax=ax)#, feature_names=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9a19",
   "metadata": {},
   "source": [
    "### Gradient Boosting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_zone_q = ReachQdf.loc[well_head_noRiv.index]\n",
    "rn =25\n",
    "print(GradientBoostingRegressor().get_params().keys())\n",
    "#choosing one well and finding the hyperparameters that best work for it \n",
    "rand_reach = np.random.randint(low=1,high=48)\n",
    "#print(rand_well)\n",
    "X_train, X_test, y_train, y_test = train_test_split(well_head_noRiv, new_zone_q[rn], test_size=0.2,random_state=29)\n",
    "loss = ['squared_error', 'absolute_error', 'huber', 'quantile']\n",
    "n_estimators = [int(x) for x in range(25,275,50)]\n",
    "subsample = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]             \n",
    "#split quality function\n",
    "split_criterion = ['squared_error', 'friedman_mse']\n",
    "splitter = ['best']\n",
    "#Number of features to consider at every split\n",
    "max_features = [1.0, 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in range(4,9)] #14 was the max depth, 3 is recommened, 10 seems high enough \n",
    "#max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "#maximum \n",
    "max_leaf_nodes = [18, 52, 86, 120, 154, 188, 222, 256, 290, 324]#[int(x) for x in range(int(np.max(y_train)**(1/2)),int(np.max(y_train)),int(np.max(y_train)/10))]\n",
    "max_leaf_nodes.append(None)\n",
    "\n",
    "#min weight fraction leaf - need to look into this one \n",
    "#min_weight_fraction_leaf = [0.0]\n",
    "learning_rate = np.arange(0.01,0.2,0.1)\n",
    "min_imp_decrease = [int(x) for x in range(5,55,10)]\n",
    "#min_imp_decrease.append(0)\n",
    "#cost complexcity prunning \n",
    "ccp_alpha = [0.0]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {#'n_estimators': n_estimators,\n",
    "                'loss' : loss,\n",
    "                'subsample':subsample,\n",
    "                'n_estimators':n_estimators,\n",
    "                'criterion' : split_criterion,\n",
    "              # 'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              # 'min_weight_fraction_leaf': min_weight_fraction_leaf,\n",
    "                'learning_rate': learning_rate,\n",
    "                'max_leaf_nodes': max_leaf_nodes,\n",
    "                'min_impurity_decrease':min_imp_decrease}\n",
    "print(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "xgr = GradientBoostingRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "xgr_random = RandomizedSearchCV(estimator = xgr, scoring = 'neg_root_mean_squared_error',param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "xgr_random.fit(X_train, y_train)\n",
    "best_pars_gr = xgr_random.best_params_\n",
    "print(best_pars_gr)\n",
    "\n",
    "#from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "best_random_gr = xgr_random.best_estimator_\n",
    "\n",
    "\n",
    "#_, ax = plt.subplots(figsize=(25, 25))\n",
    "#_ = plot_tree(best_random, ax=ax)#, feature_names=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfa2fb6",
   "metadata": {},
   "source": [
    "### Random Forest Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rn = 25\n",
    "\n",
    "new_zone_q = ReachQdf.loc[well_head_noRiv.index]\n",
    "\n",
    "print(RandomForestRegressor().get_params().keys())\n",
    "#choosing one well and finding the hyperparameters that best work for it \n",
    "rand_reach = np.random.randint(low=1,high=48)\n",
    "#print(rand_well)\n",
    "X_train, X_test, y_train, y_test = train_test_split(well_head_noRiv, new_zone_q[rn], test_size=0.2, random_state = random_state)\n",
    "n_estimators = [int(x) for x in range(25,275,50)]\n",
    "max_sample = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]             \n",
    "#split quality function\n",
    "split_criterion = ['squared_error', 'friedman_mse']\n",
    "#splitter = ['best']\n",
    "bootstrap = ['True']\n",
    "#Number of features to consider at every split\n",
    "max_features = [1.0, 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in range(4,9)] #14 was the max depth, 3 is recommened, 10 seems high enough \n",
    "#max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10,15]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "#maximum \n",
    "max_leaf_nodes = [18, 52, 86, 120, 154, 188, 222, 256, 290, 324, None]#[int(x) for x in range(int(np.max(y_train)**(1/2)),int(np.max(y_train)),int(np.max(y_train)/10))]\n",
    "#max_leaf_nodes.append(None)\n",
    "\n",
    "#min weight fraction leaf - need to look into this one \n",
    "#min_weight_fraction_leaf = [0.0]\n",
    "\n",
    "min_imp_decrease = [int(x) for x in range(5,55,10)]\n",
    "#min_imp_decrease.append(0)\n",
    "#cost complexcity prunning \n",
    "ccp_alpha = [0.0]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {#'n_estimators': n_estimators,\n",
    "                #'bootstrap' : bootstrap,\n",
    "                'max_samples':max_sample,\n",
    "                'n_estimators':n_estimators,\n",
    "                'criterion' : split_criterion,\n",
    "               #'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              # 'min_weight_fraction_leaf': min_weight_fraction_leaf,\n",
    "                'max_leaf_nodes': max_leaf_nodes,\n",
    "                'min_impurity_decrease':min_imp_decrease}\n",
    "print(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rfr = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rfr_random = RandomizedSearchCV(estimator = rfr, scoring = 'neg_root_mean_squared_error',param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rfr_random.fit(X_train, y_train)\n",
    "best_pars_rf = rfr_random.best_params_\n",
    "print(best_pars_rf)\n",
    "\n",
    "#from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "best_random_rf = rfr_random.best_estimator_\n",
    "y_pred = best_random_rf.predict(X_test)\n",
    "\n",
    "#_, ax = plt.subplots(figsize=(25, 25))\n",
    "#_ = plot_tree(best_random, ax=ax)#, feature_names=col_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
