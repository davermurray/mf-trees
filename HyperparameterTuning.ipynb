{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b9d3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b5f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the well (feature) and streamflow (target) data \n",
    "ibound = np.loadtxt('base/ibound_lyr1.txt')            \n",
    "all_well_loc = np.asarray(np.where(ibound == 1)).T\n",
    "\n",
    "#Grab the pickles of ReachQ (streamflow) and well heads\n",
    "ReachQdf = pd.read_pickle('outputs/SS_reachQ1000_test7.pkl')\n",
    "well_head_df= pd.read_pickle('outputs/heads/WellHead_SS1000test7.pkl')\n",
    "\n",
    "#Calcualting the difference in heads from the wells and in the flows from with the well and without\n",
    "NoPumpingReachQ = pd.read_pickle('outputs/SS_reachQ1000_test7NoPumping.pkl')\n",
    "No_pumping_well_head_df = pd.read_pickle('outputs/heads/WellHead_SS1000test7NoPumping.pkl')\n",
    "members = np.loadtxt(\"mf_notebooks/ModflowSfrWellOutput1000Test7NoPumping.txt\")\n",
    "\n",
    "diff_head_df = No_pumping_well_head_df - well_head_df.loc[members.astype('int')]\n",
    "diff_reachq_df = NoPumpingReachQ - ReachQdf.loc[members.astype('int')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45741194",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up the head inputs - dropping out the downstream most CH boundary and the river cells \n",
    "river_well_loc = np.where(all_well_loc[:,0] == 25)\n",
    "CHBound_loc = np.where(all_well_loc[:,1] > 47)\n",
    "drop_locs= np.concatenate((river_well_loc, CHBound_loc), axis = 1)\n",
    "\n",
    "#currently not dropping any\n",
    "drop_locs = [[]]\n",
    "\n",
    "well_head_noRiv = well_head_df.drop(drop_locs[0], axis = 1)\n",
    "streamflow = ReachQdf.loc[well_head_noRiv.index]\n",
    "\n",
    "\n",
    "well_head_noRiv = diff_head_df.drop(drop_locs[0], axis = 1)\n",
    "streamflow = diff_reachq_df.loc[well_head_noRiv.index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7062b3f",
   "metadata": {},
   "source": [
    "### Decision Tree Hyperparameter Tuning \n",
    "\n",
    "Prints out the best hyperparameter combo from the search as a dictionary - which we then use in the other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the hyperparameters\n",
    "\n",
    "#set this so we can maintain the train test split in the actual run of the model\n",
    "random_state = 29\n",
    "\n",
    "#reach number to use \n",
    "rn = 25\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(well_head_noRiv, streamflow[rn], test_size=0.2, random_state = random_state)\n",
    "\n",
    "#split quality function\n",
    "split_criterion = ['squared_error', 'friedman_mse']\n",
    "\n",
    "#Number of features to consider at every split\n",
    "max_features = [1.0]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in range(4,9)] #14 was the max depth, 3 or greater is recommened, 8 seems high enough \n",
    "#max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "\n",
    "#maximum \n",
    "maxSqrt = int(np.max(y_train)**(1/2))\n",
    "max_leaf_nodes = [int(x) for x in range(25,maxSqrt,int(maxSqrt/5))]\n",
    "max_leaf_nodes.append(None)\n",
    "\n",
    "#min weight fraction leaf - need to look into this one \n",
    "#min_weight_fraction_leaf = [0.0]\n",
    "\n",
    "min_imp_decrease = [int(x) for x in range(5,55,10)]\n",
    "#min_imp_decrease.append(0)\n",
    "#cost complexcity prunning \n",
    "ccp_alpha = [0.0]\n",
    "\n",
    "splitter = ['best']\n",
    "# Create the random grid\n",
    "random_grid = {#'n_estimators': n_estimators,\n",
    "                'criterion' : split_criterion,\n",
    "               #'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              # 'min_weight_fraction_leaf': min_weight_fraction_leaf,\n",
    "                'max_leaf_nodes': max_leaf_nodes,\n",
    "                'min_impurity_decrease':min_imp_decrease}\n",
    "                #'splitter': splitter}\n",
    "print(random_grid)\n",
    "\n",
    "#looking at how many possible parameter combinations there are \n",
    "num_par_combos = 1\n",
    "\n",
    "for i in random_grid.keys():\n",
    "    num_par_combos *= len(random_grid[i])\n",
    "print(\"Number of possible parameter combinations: \", num_par_combos )  \n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "dtr_random = RandomizedSearchCV(estimator = dtr, scoring = 'neg_root_mean_squared_error',param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=29, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "dtr_random.fit(X_train, y_train)\n",
    "\n",
    "best_pars_dt = dtr_random.best_params_\n",
    "print(\"Best DT pars: \", best_pars_dt) # This is the most important part\n",
    "\n",
    "best_random = dtr_random.best_estimator_\n",
    "y_pred = best_random.predict(X_test)\n",
    "#well head Tree NSE\n",
    "\n",
    "#label='Prediction to Test Values for Hyperparameter Tuned Tree'\n",
    "plt.scatter(y_test, y_pred)  \n",
    "outmax = np.max(y_test)\n",
    "plt.plot([0, outmax], [0, outmax], color='k', linestyle='-')\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "# plt.xlim(60,100)\n",
    "# plt.ylim(60,100)\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# _, ax = plt.subplots(figsize=(25, 25))\n",
    "# _ = plot_tree(best_random, ax=ax)#, feature_names=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afddbff8",
   "metadata": {},
   "source": [
    "### Gradient Boosting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbb608d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['alpha', 'ccp_alpha', 'criterion', 'init', 'learning_rate', 'loss', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_iter_no_change', 'random_state', 'subsample', 'tol', 'validation_fraction', 'verbose', 'warm_start'])\n",
      "{'loss': ['squared_error', 'absolute_error', 'huber', 'quantile'], 'subsample': [0.5, 0.6, 0.7, 0.8, 0.9], 'n_estimators': [25, 75, 125, 175, 225], 'criterion': ['squared_error', 'friedman_mse'], 'max_depth': [4, 5, 6, 7, 8], 'min_samples_split': [2, 5, 10, 15], 'min_samples_leaf': [1, 2, 5, 10], 'learning_rate': [0.01, 0.1, 0.2, 0.5], 'max_leaf_nodes': [50, 62, 74, 86, 98, 110, 122, None], 'min_impurity_decrease': [5, 15, 25, 35, 45]}\n",
      "Number of possible parameter combinations:  2560000\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "{'subsample': 0.6, 'n_estimators': 175, 'min_samples_split': 15, 'min_samples_leaf': 1, 'min_impurity_decrease': 25, 'max_leaf_nodes': 86, 'max_depth': 4, 'loss': 'huber', 'learning_rate': 0.1, 'criterion': 'friedman_mse'}\n"
     ]
    }
   ],
   "source": [
    "new_zone_q = ReachQdf.loc[well_head_noRiv.index]\n",
    "rn =25\n",
    "print(GradientBoostingRegressor().get_params().keys())\n",
    "#choosing one well and finding the hyperparameters that best work for it \n",
    "rand_reach = np.random.randint(low=1,high=48)\n",
    "#print(rand_well)\n",
    "X_train, X_test, y_train, y_test = train_test_split(well_head_noRiv, streamflow[rn], test_size=0.2,random_state=29)\n",
    "loss = ['squared_error', 'absolute_error', 'huber', 'quantile']\n",
    "n_estimators = [int(x) for x in range(25,275,50)] #this was found by looking at oob predictions - actually could have been lower than this\n",
    "subsample = [0.5,0.6,0.7,0.8,0.9]             \n",
    "#split quality function\n",
    "split_criterion = ['squared_error', 'friedman_mse']\n",
    "splitter = ['best']\n",
    "#Number of features to consider at every split\n",
    "max_features = [1.0, 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in range(4,9)] #14 was the max depth, 3 is recommened, 10 seems high enough \n",
    "#max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "#maximum \n",
    "maxSqrt = int(np.max(y_train)**(1/2))\n",
    "max_leaf_nodes = [int(x) for x in range(50,maxSqrt,int(maxSqrt/5))]\n",
    "max_leaf_nodes.append(None)\n",
    "#min weight fraction leaf - need to look into this one \n",
    "#min_weight_fraction_leaf = [0.0]\n",
    "learning_rate = [0.01,0.1,0.2,0.5]\n",
    "min_imp_decrease = [int(x) for x in range(5,55,10)]\n",
    "#min_imp_decrease.append(0)\n",
    "#cost complexcity prunning \n",
    "ccp_alpha = [0.0]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {#'n_estimators': n_estimators,\n",
    "                'loss' : loss,\n",
    "                'subsample':subsample,\n",
    "                'n_estimators':n_estimators,\n",
    "                'criterion' : split_criterion,\n",
    "              # 'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              # 'min_weight_fraction_leaf': min_weight_fraction_leaf,\n",
    "                'learning_rate': learning_rate,\n",
    "                'max_leaf_nodes': max_leaf_nodes,\n",
    "                'min_impurity_decrease':min_imp_decrease}\n",
    "print(random_grid)\n",
    "#looking at how many possible parameter combinations there are \n",
    "num_par_combos = 1\n",
    "\n",
    "for i in random_grid.keys():\n",
    "    num_par_combos *= len(random_grid[i])\n",
    "print(\"Number of possible parameter combinations: \", num_par_combos )  \n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "xgr = GradientBoostingRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "xgr_random = RandomizedSearchCV(estimator = xgr, scoring = 'neg_root_mean_squared_error',param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "xgr_random.fit(X_train, y_train)\n",
    "best_pars_gr = xgr_random.best_params_\n",
    "print(best_pars_gr)\n",
    "\n",
    "#from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "best_random_gr = xgr_random.best_estimator_\n",
    "\n",
    "\n",
    "#_, ax = plt.subplots(figsize=(25, 25))\n",
    "#_ = plot_tree(best_random, ax=ax)#, feature_names=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb2e09",
   "metadata": {},
   "source": [
    "### Random Forest Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02441b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bootstrap', 'ccp_alpha', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])\n",
      "{'max_samples': [0.1, 0.3, 0.5, 0.7, 0.9], 'n_estimators': [25, 75, 125, 175, 225], 'criterion': ['squared_error', 'friedman_mse'], 'max_depth': [4, 5, 6, 7, 8], 'min_samples_split': [2, 5, 10, 15], 'min_samples_leaf': [1, 2, 5, 10], 'max_leaf_nodes': [50, 74, 98, 122, None], 'min_impurity_decrease': [5, 15, 25, 35, 45]}\n",
      "Number of possible parameter combinations:  100000\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "{'n_estimators': 225, 'min_samples_split': 5, 'min_samples_leaf': 2, 'min_impurity_decrease': 25, 'max_samples': 0.9, 'max_leaf_nodes': None, 'max_depth': 7, 'criterion': 'friedman_mse'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rn = 25\n",
    "random_state = 29\n",
    "new_zone_q = ReachQdf.loc[well_head_noRiv.index]\n",
    "\n",
    "print(RandomForestRegressor().get_params().keys())\n",
    "#choosing one well and finding the hyperparameters that best work for it \n",
    "rand_reach = np.random.randint(low=1,high=48)\n",
    "#print(rand_well)\n",
    "X_train, X_test, y_train, y_test = train_test_split(well_head_noRiv, streamflow[rn], test_size=0.2, random_state = random_state)\n",
    "n_estimators = [int(x) for x in range(25,275,50)]\n",
    "max_sample = [0.1,0.3,0.5,0.7,0.9]             \n",
    "#split quality function\n",
    "split_criterion = ['squared_error', 'friedman_mse']\n",
    "splitter = ['best']\n",
    "bootstrap = ['True']\n",
    "#Number of features to consider at every split\n",
    "max_features = [1.0]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in range(4,9)] #14 was the max depth, 3 is recommened, 10 seems high enough \n",
    "#max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10,15]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "#maximum \n",
    "maxSqrt = int(np.max(y_train)**(1/2))\n",
    "max_leaf_nodes = [int(x) for x in range(50,maxSqrt,int(maxSqrt/5))]\n",
    "max_leaf_nodes.append(None)\n",
    "\n",
    "#min weight fraction leaf - need to look into this one \n",
    "#min_weight_fraction_leaf = [0.0]\n",
    "\n",
    "min_imp_decrease = [int(x) for x in range(5,55,10)]\n",
    "#min_imp_decrease.append(0)\n",
    "#cost complexcity prunning \n",
    "ccp_alpha = [0.0]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {#'n_estimators': n_estimators,\n",
    "                #'bootstrap' : bootstrap,\n",
    "                'max_samples':max_sample,\n",
    "                'n_estimators':n_estimators,\n",
    "                'criterion' : split_criterion,\n",
    "               #'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              # 'min_weight_fraction_leaf': min_weight_fraction_leaf,\n",
    "                'max_leaf_nodes': max_leaf_nodes,\n",
    "                'min_impurity_decrease':min_imp_decrease}\n",
    "print(random_grid)\n",
    "\n",
    "#looking at how many possible parameter combinations there are \n",
    "num_par_combos = 1\n",
    "\n",
    "for i in random_grid.keys():\n",
    "    num_par_combos *= len(random_grid[i])\n",
    "    \n",
    "print(\"Number of possible parameter combinations: \", num_par_combos ) \n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rfr = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rfr_random = RandomizedSearchCV(estimator = rfr, scoring = 'neg_root_mean_squared_error',param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rfr_random.fit(X_train, y_train)\n",
    "best_pars_rf = rfr_random.best_params_\n",
    "print(best_pars_rf)\n",
    "\n",
    "#from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "best_random_rf = rfr_random.best_estimator_\n",
    "y_pred = best_random_rf.predict(X_test)\n",
    "\n",
    "#_, ax = plt.subplots(figsize=(25, 25))\n",
    "#_ = plot_tree(best_random, ax=ax)#, feature_names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed0f20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
